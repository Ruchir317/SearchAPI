{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACT = \"Is it true the rumor that all Social Security beneficiaries will have to go to a location to verify their identity to receive their monthly check\"\n",
    "\n",
    "def google_search(query, api_key, cse_id, num=12):\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=query, cx=cse_id, num=num).execute()\n",
    "    return res.get('items', [])\n",
    "\n",
    "def summarize_article(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        article.nlp()\n",
    "        return article.summary\n",
    "    except Exception as e:\n",
    "        return None  # Mark invalid\n",
    "\n",
    "def filter_top_articles(articles, top_k=8):\n",
    "    # Score and select the best\n",
    "    scored = []\n",
    "    for a in articles:\n",
    "        summary = a.get(\"summary\", \"\")\n",
    "        if summary and len(summary.strip()) >= 200:\n",
    "            scored.append((len(summary.strip()), a))\n",
    "    # Sort by length of summary (basic quality metric)\n",
    "    print(scored)\n",
    "    # scored.sort(reverse=True)\n",
    "    return [a for _, a in scored[:top_k]]\n",
    "\n",
    "def fact_checker_and_save(fact, json_filename=\"fact_results.json\"):\n",
    "    print(f\"üîç Searching: {fact}\")\n",
    "    raw_articles = google_search(fact, API_KEY, CSE_ID, num=10)\n",
    "\n",
    "    enriched = []\n",
    "    for result in raw_articles:\n",
    "        url = result.get(\"link\")\n",
    "        title = result.get(\"title\")\n",
    "        snippet = result.get(\"snippet\")\n",
    "        summary = summarize_article(url)\n",
    "\n",
    "        if summary:\n",
    "            enriched.append({\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"snippet\": snippet,\n",
    "                \"summary\": summary\n",
    "            })\n",
    "\n",
    "        time.sleep(1)  # politeness delay\n",
    "\n",
    "    best_articles = filter_top_articles(enriched, top_k=8)\n",
    "\n",
    "    result = {\n",
    "        \"fact\": fact,\n",
    "        \"articles\": best_articles\n",
    "    }\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Saved top {len(best_articles)} articles to {json_filename}\")\n",
    "\n",
    "# Run it\n",
    "fact_checker_and_save(FACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import re\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load your JSON file (Fact + Articles)\n",
    "def load_fact_json(json_path):\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Prepare structured prompt input based on fact + summaries\n",
    "def build_input_from_json(fact_data):\n",
    "    fact = fact_data[\"fact\"]\n",
    "    articles = fact_data[\"articles\"]\n",
    "\n",
    "    input_str = f\"**Factual Claim:**\\n{fact}\\n\\n\"\n",
    "    input_str += \"**Relevant Articles:**\\n\"\n",
    "\n",
    "    for i, article in enumerate(articles, 1):\n",
    "        input_str += f\"\\nArticle {i}:\\n\"\n",
    "        input_str += f\"- **Title:** {article['title']}\\n\"\n",
    "        input_str += f\"- **URL:** {article['url']}\\n\"\n",
    "        input_str += f\"- **Snippet:** {article['snippet']}\\n\"\n",
    "        input_str += f\"- **Summary:** {article['summary']}\\n\"\n",
    "\n",
    "    return input_str\n",
    "\n",
    "# Run Gemini API with the filled-in prompt\n",
    "def run_fact_check(prompt_text, fact_text, full_output_file=\"full_output.json\", parsed_output_file=\"parsed_output.json\"):\n",
    "    client = genai.Client(\n",
    "        api_key=API_KEY,\n",
    "    )\n",
    "\n",
    "    model = \"gemini-2.0-flash\"\n",
    "\n",
    "    contents = [\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part.from_text(text=prompt_text)],\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    generate_content_config = types.GenerateContentConfig(\n",
    "        temperature=0,\n",
    "        response_mime_type=\"text/plain\",\n",
    "        system_instruction=[\n",
    "            types.Part.from_text(text=\"\"\"You are an automated fact-verification assistant. You will be given a structured input that contains:\n",
    "\n",
    "- A factual claim at the top\n",
    "- A list of summarized news articles that are relevant to that claim. Each article includes:\n",
    "  - Title\n",
    "  - URL\n",
    "  - A short snippet\n",
    "  - A full summary of the article's content\n",
    "\n",
    "Your task is to analyze the provided article summaries and assess how well they support or contradict the factual claim.\n",
    "\n",
    "You must provide the following in your response:\n",
    "\n",
    "1. **Verdict** ‚Äì Choose only **\"True\"** or **\"False\"** based on your evaluation of the evidence.\n",
    "\n",
    "2. **Collective Summary** ‚Äì A short synthesis of what the articles collectively say. Focus on:\n",
    "   - Overall agreement or disagreement with the fact\n",
    "   - Whether the evidence is strong, partial, mixed, or weak\n",
    "   - Any outliers or conflicting perspectives\n",
    "\n",
    "3. **Reasoning** ‚Äì Provide justification for your verdict in 2‚Äì4 sentences. Refer directly to article patterns (e.g., \"5 out of 7 articles support the claim that CO2 emissions are a leading cause of accelerated climate change\").\n",
    "\n",
    "4. **Sources Summary** ‚Äì Bullet-point list of all article titles with a one-line comment on how each relates to the fact (e.g., supports, contradicts, or provides background).\n",
    "\n",
    "Formatting Rules:\n",
    "- Start your output with: **Verdict: True** or **Verdict: False**\n",
    "- Be objective and analytical ‚Äî do not speculate\n",
    "- Use markdown for readability\n",
    "- Keep your total response under 200 words unless otherwise instructed\n",
    "- Do not perform live search or external lookups ‚Äî rely only on the provided content\n",
    "\n",
    "Your role is to emulate a professional fact-checking analyst using summarized content from multiple sources to reach a binary decision.\n",
    "\"\"\")\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    print(\"üîç Submitting to Gemini...\\n\")\n",
    "\n",
    "    full_response = \"\"\n",
    "    for chunk in client.models.generate_content_stream(\n",
    "        model=model,\n",
    "        contents=contents,\n",
    "        config=generate_content_config,\n",
    "    ):\n",
    "        print(chunk.text, end=\"\")\n",
    "        full_response += chunk.text\n",
    "\n",
    "    # Save the full markdown-style output\n",
    "    with open(full_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"fact\": fact_text,\n",
    "            \"gemini_output\": full_response.strip()\n",
    "        }, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Parse response into parts using regular expressions\n",
    "    parsed = {\n",
    "        \"fact\": fact_text,\n",
    "        \"verdict\": extract_section(\"Verdict\", full_response),\n",
    "        \"collective_summary\": extract_section(\"Collective Summary\", full_response),\n",
    "        \"reasoning\": extract_section(\"Reasoning\", full_response),\n",
    "        \"sources_summary\": extract_bullet_list(\"Sources Summary\", full_response)\n",
    "    }\n",
    "\n",
    "    # Save parsed version\n",
    "    with open(parsed_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(parsed, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n\\n‚úÖ Output saved to {full_output_file} and {parsed_output_file}\")\n",
    "\n",
    "def extract_section(header, text):\n",
    "    pattern = rf\"\\*\\*{header}:\\*\\*\\s*(.*?)(?=\\n\\*\\*|$)\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def extract_bullet_list(header, text):\n",
    "    section = extract_section(header, text)\n",
    "    if section:\n",
    "        bullets = re.findall(r\"[*\\-]\\s+(.*)\", section)\n",
    "        return [b.strip() for b in bullets]\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver function\n",
    "def verify_fact_from_file(json_path):\n",
    "    data = load_fact_json(json_path)\n",
    "    fact = data[\"fact\"]\n",
    "    filled_prompt = build_input_from_json(data)\n",
    "    run_fact_check(filled_prompt, fact)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual path to the JSON file\n",
    "    verify_fact_from_file(\"fact_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google\n",
    "print(google.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"data/politifact_factcheck_data.json\"\n",
    "\n",
    "def print_sample_lines(filepath, max_lines=10):\n",
    "    print(f\"\\nüîç Reading up to {max_lines} lines from: {filepath}\\n\")\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_lines:\n",
    "                    break\n",
    "                try:\n",
    "                    parsed = json.loads(line.strip())\n",
    "                    print(f\"[{i}] statement: {parsed.get('statement')}\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"[{i}] ‚ùå JSON error: {e}\")\n",
    "                    print(f\"Raw line: {line}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found: {filepath}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_sample_lines(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
